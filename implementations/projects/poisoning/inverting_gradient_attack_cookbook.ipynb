{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverting gradient attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.modules.loss import _Loss, CrossEntropyLoss\n",
    "from torch.optim import Optimizer, SGD, Adam, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, TensorDataset\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "from image_classification.utils import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet-18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For quick prototyping, also consider using `ShuffleNetV2`, a 300M-parameter model that is much smaller than `ResNet18`. Experiments can be made on both models or on only one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_classification.models import ResNet18, ShuffleNetV2\n",
    "from image_classification.datasets import cifar10_train_test, cifar100_train_test\n",
    "from image_classification.nn import train_loop, train_val_loop, test_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to 10 for CIFAR-10, 100 for CIFAR-100\n",
    "num_classes = 10\n",
    "\n",
    "# The images are already normalized by theses datasets\n",
    "if num_classes == 10:\n",
    "    get_train_test = cifar10_train_test\n",
    "elif num_classes == 100:\n",
    "    get_train_test = cifar100_train_test\n",
    "else:\n",
    "    raise ValueError(f\"Can't find CIFAR dataset with {num_classes} classes\")\n",
    "print(f\"Loading CIFAR-{num_classes}\")\n",
    "\n",
    "training_data, test_data = get_train_test(root='data')\n",
    "N_test = len(test_data)\n",
    "N_val = len(training_data) // 10\n",
    "N_aux = N_val\n",
    "N = len(training_data) - N_val - N_aux\n",
    "# This works since training data is already shuffled\n",
    "training_data, val_data, aux_data = training_data.split([N, N_val, N_aux])\n",
    "\n",
    "batch_size = 100\n",
    "N, N_val, N_aux, N_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(training_data, batch_size, drop_last=True)\n",
    "val_loader = DataLoader(val_data, batch_size, drop_last=True)\n",
    "aux_loader = DataLoader(aux_data, batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "weight_decay = 5e-4\n",
    "# For learning rate scheduling\n",
    "max_lr = 0.1\n",
    "\n",
    "epochs = 6\n",
    "steps_per_epoch = N // batch_size\n",
    "\n",
    "lr_sched_params = dict(max_lr=max_lr, epochs=epochs, steps_per_epoch=steps_per_epoch)\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "top_k = {10: 1, 100: 5}[num_classes]\n",
    "\n",
    "metric = MulticlassAccuracy(num_classes=num_classes, top_k=top_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_optimizer(model: nn.Module, opt_name='adamw', lr=lr, weight_decay=weight_decay, **kwargs) -> Optimizer:\n",
    "    cls = {'sgd': SGD, 'adam': Adam, 'adamw': AdamW}[opt_name]\n",
    "    return cls(model.parameters(), lr=lr, weight_decay=weight_decay, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverting gradient attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import image_classification.gradient_attack\n",
    "reload(image_classification.gradient_attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_classification.gradient_attack import (\n",
    "    GradientAttack,\n",
    "    GradientEstimator, OmniscientGradientEstimator, ShadowGradientEstimator,\n",
    "    SampleInit, SampleInitRandomNoise,\n",
    "    GradientInverter,\n",
    "    Schedule, NeverUpdate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ResNet18(num_classes=num_classes).to(device)\n",
    "opt = make_optimizer(net, opt_name='adam', lr=lr)\n",
    "# Pretrain the model to make it learn the features\n",
    "mini_train_set = Subset(training_data, np.arange(N_aux))\n",
    "mini_train_loader = DataLoader(mini_train_set, batch_size)\n",
    "train_val_loop(\n",
    "    net, mini_train_loader, val_loader,\n",
    "    criterion, opt,\n",
    "    epochs=10, # Overfit on the first `N_aux` examples of the training data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poisoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_classification.datasets import UpdatableDataset\n",
    "from image_classification.nn import MetricLogger\n",
    "\n",
    "def train_epoch_with_poisons(\n",
    "        model: nn.Module,\n",
    "        dataloader: DataLoader,\n",
    "        criterion: _Loss,\n",
    "        optimizer: Optimizer,\n",
    "        inverter: GradientInverter,\n",
    "        alpha_poison=0.2,\n",
    "        keep_pbars=True,\n",
    "    ) -> tuple[UpdatableDataset, MetricLogger]:\n",
    "    model.train()\n",
    "    logger = MetricLogger(\n",
    "        metric,\n",
    "        device=device,\n",
    "        desc='Train loop', total=len(dataloader.dataset), keep_pbars=keep_pbars,\n",
    "    )\n",
    "    poison_set = UpdatableDataset()\n",
    "\n",
    "    for X, y in dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        logits = model(X)\n",
    "        # TODO: handle losses that don't reduce\n",
    "        loss = criterion(logits, y)\n",
    "        # TODO: backpropagate on each loss element (and model.zero_grad() every time)\n",
    "        loss.backward()\n",
    "\n",
    "        # --- poisoning attack\n",
    "        X_p, y_p = inverter.attack(model, criterion)\n",
    "        poison_set.append(X_p, y_p)\n",
    "\n",
    "        logits_p = model(X_p.unsqueeze(0))\n",
    "        loss_p = alpha_poison * criterion(logits_p, y_p.unsqueeze(0))\n",
    "        # This adds to `loss` model gradients due to gradient accumulation\n",
    "        loss_p.backward()\n",
    "        # ---\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # FIXME: does not include X_p, y_p, logits_p, loss_p\n",
    "        # TODO: log loss on poisons\n",
    "        # TODO: display some poisons\n",
    "        logger.compute_metrics(X, y, logits, loss.item())\n",
    "    \n",
    "    logger.finish()\n",
    "    return poison_set, logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_with_poisons(\n",
    "        model: nn.Module,\n",
    "        train_loader: DataLoader,\n",
    "        val_loader: DataLoader,\n",
    "        criterion: _Loss,\n",
    "        optimizer: Optimizer,\n",
    "        epochs: int,\n",
    "        inverter: GradientInverter,\n",
    "        alpha_poison=0.05,\n",
    "        metric=metric,\n",
    "    ) -> TensorDataset:\n",
    "    poison_set = UpdatableDataset()\n",
    "    for epoch in trange(epochs, desc='Train epochs', unit='epoch', leave=True):\n",
    "        poison_set_epoch, _ = train_epoch_with_poisons(\n",
    "            model, train_loader,\n",
    "            criterion, optimizer,\n",
    "            inverter, alpha_poison=alpha_poison\n",
    "        )\n",
    "        poison_set.extend(poison_set_epoch)\n",
    "        test_epoch(model, val_loader, criterion, keep_pbars=True, metric=metric)\n",
    "    return poison_set.to_tensor_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orthogonal Gradient inverting attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = OmniscientGradientEstimator()\n",
    "sample_init = SampleInitRandomNoise(aux_data)\n",
    "inverter = GradientInverter(\n",
    "    GradientAttack.ORTHOGONAL,\n",
    "    estimator,\n",
    "    steps=5,\n",
    "    sample_init=sample_init,\n",
    "    tv_coef=0.0,\n",
    "    lr=0.3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ShuffleNetV2().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD is more vulnerable to gradient attacks\n",
    "net = ResNet18(num_classes=num_classes).to(device)\n",
    "opt = make_optimizer(net, opt_name='sgd', lr=lr, weight_decay=0.0)\n",
    "train_loop_with_poisons(\n",
    "    net, train_loader, val_loader,\n",
    "    criterion, opt,\n",
    "    epochs,\n",
    "    inverter, alpha_poison=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training progress is slowed down by a lot, however accuracy does not drop as much as with Gradient Ascent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Ascent inverting attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = OmniscientGradientEstimator()\n",
    "sample_init = SampleInitRandomNoise(aux_data)\n",
    "inverter = GradientInverter(\n",
    "    GradientAttack.ASCENT,\n",
    "    estimator,\n",
    "    steps=5,\n",
    "    sample_init=sample_init,\n",
    "    tv_coef=0.0,\n",
    "    lr=0.3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD is more vulnerable to gradient attacks\n",
    "net = ResNet18(num_classes=num_classes).to(device)\n",
    "opt = make_optimizer(net, opt_name='sgd', lr=lr, weight_decay=0.0)\n",
    "train_loop_with_poisons(\n",
    "    net, train_loader, val_loader,\n",
    "    criterion, opt,\n",
    "    epochs,\n",
    "    inverter, alpha_poison=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: test with more poison steps or different lr for poison optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lower poisoning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ResNet18(num_classes=num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = make_optimizer(net, opt_name='sgd', lr=lr, weight_decay=0.0)\n",
    "train_loop_with_poisons(\n",
    "    net, train_loader, val_loader,\n",
    "    criterion, opt,\n",
    "    epochs,\n",
    "    inverter, alpha_poison=0.05,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Adam optimizer for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ResNet18(num_classes=num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam regularizes the parameters so it is more robust to gradient attacks\n",
    "opt = make_optimizer(net, opt_name='adam', lr=lr)\n",
    "train_loop_with_poisons(\n",
    "    net, train_loader, val_loader,\n",
    "    criterion, opt,\n",
    "    epochs,\n",
    "    inverter, alpha_poison=0.2,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, test accuracy jumps from 40 % to 60 % in one epoch. Explanation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine unlearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_classification.unlearning import (\n",
    "    gradient_descent, gradient_ascent, neg_grad_plus, unlearning_last_layers, scrub\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ResNet18(num_classes=num_classes).to(device)\n",
    "opt = make_optimizer(net, opt_name='sgd', lr=lr)\n",
    "forget_set = train_loop_with_poisons(\n",
    "    net, train_loader, val_loader,\n",
    "    criterion, opt,\n",
    "    epochs,\n",
    "    inverter, alpha_poison=0.2,\n",
    ")\n",
    "forget_loader = DataLoader(forget_set, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class Unlearning(Enum):\n",
    "    GRADIENT_DESCENT = 0\n",
    "    GRADIENT_ASCENT = 1\n",
    "    NOISY_GRADIENT_DESCENT = 2\n",
    "    NEG_GRAD_PLUS = 3\n",
    "    CFK = 4\n",
    "    EUK = 5\n",
    "    SCRUB = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unlearn(\n",
    "        net: nn.Module,\n",
    "        # the train loader is not poisoned (poisons are generated continuously)\n",
    "        train_loader: DataLoader,\n",
    "        forget_loader: DataLoader,\n",
    "        criterion: _Loss,\n",
    "        method: Unlearning,\n",
    "    ):\n",
    "    unlearner = deepcopy(net)\n",
    "    \n",
    "    match method:\n",
    "        case Unlearning.GRADIENT_DESCENT:\n",
    "            opt = make_optimizer(unlearner, opt_name='sgd', lr=lr)\n",
    "            gradient_descent(\n",
    "                unlearner, train_loader, val_loader,\n",
    "                criterion, opt, epochs=1, keep_pbars=False\n",
    "            )\n",
    "        case Unlearning.GRADIENT_ASCENT:\n",
    "            opt = make_optimizer(unlearner, opt_name='sgd', lr=1e-5)\n",
    "            gradient_ascent(\n",
    "                unlearner, train_loader, val_loader,\n",
    "                criterion, opt, epochs=1, keep_pbars=False\n",
    "            )\n",
    "        case Unlearning.NEG_GRAD_PLUS:\n",
    "            opt = make_optimizer(unlearner, opt_name='sgd', lr=lr)\n",
    "            for epoch in trange(10, desc='NegGrad+ epochs', unit='epoch', leave=True):\n",
    "                neg_grad_plus(\n",
    "                    unlearner, train_loader, forget_loader,\n",
    "                    criterion, opt, keep_pbars=False\n",
    "                )\n",
    "        case Unlearning.EUK:\n",
    "            opt = make_optimizer(unlearner, opt_name='adam', lr=lr)\n",
    "            with unlearning_last_layers(unlearner, 6, 'euk'):\n",
    "                train_loop(unlearner, train_loader, criterion, opt, epochs=1)\n",
    "        case Unlearning.SCRUB:\n",
    "            opt = make_optimizer(unlearner, opt_name='adam', lr=lr)\n",
    "            scrub(\n",
    "                net, unlearner, train_loader, forget_loader, criterion, opt,\n",
    "                max_steps=1, steps=1, keep_pbars=False,\n",
    "            )\n",
    "    \n",
    "    return unlearner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No poisoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: quantify effect of poisoning in terms of loss recovery effort (epochs). Can the model ever recover from poisoning with enough steps?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_net = ResNet18(num_classes=num_classes).to(device)\n",
    "opt = make_optimizer(clean_net, opt_name='sgd', lr=lr)\n",
    "train_val_loop(\n",
    "    clean_net, train_loader, val_loader,\n",
    "    criterion, opt,\n",
    "    epochs=epochs,\n",
    "    metric=metric,\n",
    ");\n",
    "test_epoch(clean_net, val_loader, criterion, keep_pbars=True, metric=metric);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No unlearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_epoch(net, val_loader, criterion, keep_pbars=True, metric=metric);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlearner = unlearn(net, train_loader, forget_loader, criterion, Unlearning.GRADIENT_DESCENT)\n",
    "test_epoch(unlearner, val_loader, criterion, keep_pbars=True, metric=metric);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NegGrad+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlearner = unlearn(net, train_loader, forget_loader, criterion, Unlearning.NEG_GRAD_PLUS)\n",
    "test_epoch(unlearner, val_loader, criterion, keep_pbars=True, metric=metric);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EUk ($k = 6$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlearner = unlearn(net, train_loader, forget_loader, criterion, Unlearning.EUK)\n",
    "test_epoch(unlearner, val_loader, criterion, keep_pbars=True, metric=metric);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although more extensive testing is required, unlearning methods do not fully restore accuracy, at least not with only one epoch. Why is gradient descent enough?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remaining tasks\n",
    "\n",
    "- Refactoring\n",
    "- Compare with gradient attacks\n",
    "- Compare with results from article\n",
    "- Use `Conv16` model for prototyping\n",
    "- _Little is Enough_ attack (requires efficient gradient stddev estimation)\n",
    "- Mean gradient estimation with auxiliary dataset\n",
    "- Testing against unlearning\n",
    "- Testing with different configs (optimizer, number of epochs, batch size, models)\n",
    "- Quantify results of data poisoning in terms of slowdown (x% -> y% accuracy = z training epochs)\n",
    "- Suggest other Hessian-based attacks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
